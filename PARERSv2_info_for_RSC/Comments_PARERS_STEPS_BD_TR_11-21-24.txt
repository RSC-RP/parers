
#### PARERSv2 as of 8-2-24  but with BD's change 10/14 and new BD comments updated or added 10/15####
####The new BD comments will help carefully define each step so we can make a list of what to QC.

# Part 1: input the necessary information for points 1-7

#1. Define the path to your R1 read files
#1. Define the path to your R1 read files

#2. Define the path to your R2 read files

#3. Add all your R1 and R2 variables to pairwise lists


#4. Make a list of your cell line names (in quotes) that corresponds to the order of your R1 and R2 lists


#5. Define your control sample using its mutant name


#6. Define the path to your input file. This file contains the information for which gene to analyze, new folder name, path to other programs etc


#7. Provide the path to the FASTA file containing the expected pre-edited and edited amplicons





## Part 2: process input information that will apply to all cell lines ##

# 1. load necessary packages
# import re
# import csv
# import pandas as pd
# from Bio import SeqIO
# from docx import Document
# from docx.shared import RGBColor
# from docx.shared import Pt
# from collections import Counter
# from Bio.Align import PairwiseAligner
# from Bio.Seq import Seq
# import subprocess
# import gzip
# from Bio import AlignIO
# import os
# from datetime import datetime
# from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
# import locale
# import sys
# import xlsxwriter
# import tempfile

# 2. open the input file
    # Read each line of the file
   
# 3. extract each piece of information from the input file and assign each a variable name
# default to depth for now, can change to difference as needed
#. 3a: I'm not quite sure what the code below does or it it's part of step 3. BD 10/15/24 --> this step just makes sure all ouput directories from this line on have a "/" as the last 
	character so if the user did not include it in their input file it wont cause downstream issues. And if they do include it the line does nothing 
# 4. create folder to store the data for all cell lines and notify the user if the folder already exists. This step references info from the input file
# 5. create a directory for the temp files that is deleted at the end of the run. The temp files are large so just trying to help with storage and general tidyness
# 6. find the index for your control sample. Default set to first sample in the list
# 7. load gene sequences from the input pre-edited and edited amplicons input file
	- One example of an overly complicated loop (and function where one isn't eeded)
# 8. Retrieve the pre-edited and edited gene sequences for the gene you specified in the input file
#9. checks that possible genes (in gene_names) has a match in actual gene_sequences and extracts the associated sequence
         # finds the sequence in gene_sequences FASTA file that has the name specified in gene_names
#10. compare primer sequences with pe and fe versions of the gene to see if they match. This also is dealing with the amplicons input file
 # value will be updated as long as it is found in the gene sequences FASTA
## this is a pretty convoluted way of achieving this step 
#looking for the edited sequence, everything indented on this will execute on the edited sequence only
#stores fe sequence found in the list containing all sequences, found by name
                #if primer is found, if returns the index of first occurrence, otherwise returns a -1
                #second part in the ( ) searches for occurrences of the primer after the first, which is why you need the length of the primer
                #if primers are not found starting at the index after the length of the first primer found, then -1 is returned
                #if the primer is not found multiple times within the gene sequence then the next chunk is executed
             #returns position of place where primer first matches gene sequence
                 #this just returns same value as regular fe_sequence when using truncated A6 sequence- source of problem?
# 11. get T stripped sequence from the amplicons pre-edited sequence
# 12. get primer lengths from input
#### Part 3: Start analyzing the data by starting the loop for each cell line ####
# 3.1 make datasets that will populate with different types of data used for graphs and outputs later on and create sub-folder for the cell line within the main folder (specified by input)
# 3.2 This is the start of a very long loop that is supposed to go through and analyze each set of data beginning with the 'R1 and R2' FASTQ input files
    # 3.3 create an empty folder to store all output files. This step will notify user if the folder already exists
    # 3.4 Make new data sets using the information from the input file.  Data will populate in these
    # 3.4.5 determine the sequence run identifier string. Need this so we can accurately extract sequences from the fastq later
    #3.5 IMPORTANT! This is where commands are specified for running BBMerge, which requires java and is needed to merge the R1 and R2 FASTQs into a new FASTQ
    # related info: BBMerge settings from strictest to loosest: xstrict, ustrict, vstrict, strict, default, loose, vloose, uloose, xloose
    # related info: since we currently do not have one specified that means we are using the default setting.
    # 3.5a run the command to execute BBMerge
    #3.6 this command takes the FASTQ output from BBMerge and converts it into the FASTA that is used for downstream analysis. bbmerge does not seem to have an ability to ouput a fasta,
	chose to save the merged reads in fasta format though because it is more readable by eye and takes less storage
    # 3.7 This uses the Biopython SeqIO.parse function to take the input data set ("final_Fasta" here and format it as fasta, and "returns an iterator giving SeqRecord objects" per Biopython doc
    #3.8 this next part is to count the number of reads in the R1 input FASTQ and the length of the merged reads FASTA to determine the % merged
    # 3.9 this step is making a new list of all the sequences in the merged reads list and turing something into a dictionary???
    #### when you use SeqIO to read a fasta file it returns LOTS of information, most of which we dont need. So this code extracts just the sequence and run name and stores them in a dictionary
    #### so they are easy to access and makes the code a bit more readable than using list indexing
    # 3.9a this also seems to be RCing seqs.  I'm not quite sure what this output should look like, check back with Tyler?
    #### with the way the primers were designed the sequences need to be reverse complemented prior to matching to primers. The sequences came back in "reverse" orientation where if we wanted to
    #### primer match our forward and reverse primers would need to be reverse complemented, but I thought it would be easier to just write code here to RC the seqs so the user doesnt need to think about it
    # 3.10 reverse complement all sequences that are in reverse orientation because for some reason our MiSeq data has some reads in the opposite orientation
    # 3.11 filter out all sequences that do not contain the forward and reverse primer and notify user
    ### 3.12 here starts primer deduplication
    # 3.12a trim off forward barcodes
    # 3.12b trim off reverse barcodes
    # 3.12c concatenate the barcodes
    # 3.12d count the occurrence of each concatenated barcode
    # 3.12e store duplicate barcodes and their counts in a dictionary
    # sort by highest count
    # 3.12f update the filtered_seqs_with_barcodes dictionary by adding concatenated barcodes to filtered sequences dictionary
    # 3.12g store all those with duplicate barcode in new list
    # 3.12h set up a dictionary assigning an arbitrary number to each unique concatenated barcode
    # 3.12i add the barcode assignment number to the dictionary and add a 0 if the barcode is not present in the list of duplicate barcodes
    # 3.12j if the barcode identifier has been observed add the seq that was just iterated over to that list. If not, create a new list for that barcode
    # 3.12k order the dictionary so those with the longer list values are higher.  ASK ABOUT THIS STEP AND SOME FOLLOWiNG
    # 3.12l convert the dictionary to a list 
	- I was more comfortable using lists when I first started which is why this step is here
    # 3.12m remove sequences with non-duplicate barcodes from the list. List with non-duplicates will always be longest which is why we simply remove the first item
    # 3.12n determine the similarity between sequences that have matching barcodes
    # 3.12o extract all those sequences and barcodes that have different middle sequences
    # 3.12p create a function that will create pairwise alignments of all sequences in a list and select out the alignment with the best score.
    # uses the Pairwise Aligner tool from Biopython that scores alignments based on the Needleman-Wunsch algorithm
         # takes the alignment with the highest score
    # 3.12q calculates the percent identity between aligned sequences using a base to base comparison
    # 3.12r take the sequences from the list containing sequences with duplicate barcodes and create alignments and percent identities using the functions above
    # 3.12s calculate the average percent identity for all sequences that have common barcodes
    # 3.12u write out barcodes and duplicate counts to a csv file
    # 3.12v isolate unique barcodes using key value pairs
	- I think we can improve this step. We've been experience minor consistency issues across runs
	- The idea was to take sequences effectively at random to reduce bias in instances where there are duplicate barcodes
    # 3.12w calculate percent of sequences retained after filtering out sequences with duplicate barcodes
    # 3.12x trim barcodes off of the sequences and store sequences and notify user. After this step we no longer have any need for barcodes
    ### 3.13 T-stripping begins ###
    # 3.13a generate list of all sequences that passed primer binning
    ## 3.13b making a list of all the T positions in each sequence
    ### 3.13c this is a loop that iterates through each seq in primer_binned_seqs_list
        ## 3.13c1 Compare non-T seq with ref seq and filter
    #3.13d this step calulates the length of the lists before and after T-stripping step and notifys user how many sequences remain
    ## 3.13e making a list of all the T positions in each T strip filtered sequence
    # stores the index of each t occurrence in each sequence that made it through T strip filtering
    ### 3.13f write to output file
    ### this particuar loop needs to be rewritten because it severely overcomplicates things. filtered_seqs already has Ts in it. if we want to write all the filtered seqs out to a fasta we can
    ### just do that, we dont need to check the index and add Ts if our t positions list says so...
        # used in a t count function later
    # 3.13g Writes out the non-T mismatch sequences
    # reject seqs is in the same format as filtered seqs and notice how much simpler this loop is
    # 3.13h Put non-T reject sequences into a list
    ## 3.13h info: open the fasta file and put it into a list, in string form
           # only taking the sequences that passed the primer binning test
    # 3.13i strip the Ts from the non-T reject sequences
    # 3.13j count the non-T rejects
    # 3.13k sort the counted non-T rejects where higher counts are at the top of the list
    # 3.13l calculate the percentages for each sequence
    # 3.13m select out the top ten sequences
    # 3.13o convert to a list
    # just adding a number here so we can track each item later. This is because when muscle does the alignments it includes some number as the name (based on length maybe?) so if sequences have the 
	same value in that name they will appear to be the same, so adding the arbitrary value helps us maintain that sequence's poition in the list based on abundance
    # 3.13p add the percentages to the list
    # 3.13q write out the top non-T reject sequences to a FASTA
    # 3.13r set syntax to use MUSCLE alignment for the top A/C/G mismatch seqs
    # 3.13s run MUSCLE from the command line
    # 3.13t Parse the output aligned sequences
        # 3.13t1 Find the sublist corresponding to the sequence ID
                # Assuming seq_id is the first element in each sublist
                # Append the aligned sequence as a new item to the sublist
                # exit the sub-loop once the sublist is found
    # 3.13u add the percentages to the top_rejects_list as a new item for each sublist
    # 3.13v separate the aligned version of amplicon non-T seq
    ### 3.13w writing out nont_rejects to a word doc
    # 3.13w1 write in the amplicon non-T sequence
    # 3.13w2 Write the alignment information to the document
                 # red for G
                 # yellow for A
                 # blue for C
    # 3.14 make a dictionary to condense the filtered seqs down to identical reads with frequency counts
	- could replace this to use the Counter() function like we did for barcodes
    ### 3.15 sort the dictionary first by frequency then defer to length of sequence if necessary.  These next parts are for making the 'condensed identical reads' csv files
          # get total number of sequences
    # determine the percentage each identical read occupies
    # 3.16 get length of each observed read
    # 3.17 sample names here
    # 3.18 pull out all identical reads that appear more than once
    # 3.19 pull out all reads that occur only once
    # 3.20 generate the condensed identical reads CSV output file
    # 3.21 generate a condensed identical reads file for just those sequences that appear more than once
    # 3.22 generate a condensed identical reads file for just those sequences that appear only once
    ############# 3.23   Here begins the functions used to make the data sets that will be used for bubble plots and bar graphs   #############
    #3.23a the 'intermed5' file notes the number of Ts following each A/C/G.
           # will contain the number of Ts found after each A/C/G nucleotide, categorized by A/C/G
           # uses fasta file containing sequences that passed all filters
           ## specify name of output file
                # write out the name of the sequence being checked in this loop iteration
                # finds indexes in string of bases matching A, C or G LW
                # only runs loop for where A, C or G's are located in the string/read LW
    # 3.23b:  The intermed7 file is a precursor to the final csv that is used to make the bubble plots.  It is generated using the intermed5 file
          #this is the bubbleplot csv output but without normalized counts, order, or annotation
          #BD's understanding: this is different than the Tcountseq because it replaces the *s with integers?
                             # Tcountan seems to condense the information from intermed5. Instead of listing the T counts at each site for each individual sequence, it displays every identified
                             # editing site across all sequences, displays the unique T counts at each site, then lists how many instances of each unique T count there is across all sequences -Tyler
            # write header in intermed7 file
                # If the line starts with a nucleotide, extract the counts for that nucleotide
                        # remove non-numeric characters
            # Write the results to the output file
                         # new intermed7 LW
    #3.23c the intermed8 file seems to be an updated version of the intermed7 file that has updated indices that can be used by R for bubbleplots later
        ## yes, this rearranges the previous file so it is in the same order as the non-t seq instead of the random bracket order from intermed5
         # 3.23c1 copies the pe T-stripped sequence from amplicon_non_t_seq to yawg
        # we can delete yawg and just loop through each nuc in amplicon_non_t_seq directly (so no need for step 3.23c1)
         #3.23c2 make a dictionary of A/C/G nucleotides with values starting at 0
        #3.23c3 iterate through the nucleotides in the sequence and add 1 to the counter for each nucleotide to assign positions to the A/C/G nts
            # append the current count of the current nucleotide to the running list
            #example output for sequence [A,C,G,A] would be [1,1,1,2]
            # normalize starting point to 0 instead of 1
        # 3.23c4 create new dataframe with the normalized position, nucleotide, and order number
                                     # will need to update this to amplicon_non_t_seq when we change yawg
        ### 3.23c5 merge the order DataFrame with the original DataFrame to extract the corresponding rows
        ### 3.23c6 sort the merged DataFrame by the "Order" column
        ## 3.23c7 write the extracted rows to a new CSV file, excluding the "Order" column
    # 3.24 defines a function that replaces Ts with asterisks then counts the asterisks. Should just count Ts and not replace with asterisks...
           # 3.24a make another T-stripped sequence
        # 3.24b create a list to store the counts of asterisks after each non-T nucleotide
        # 3.24c count and store the asterisks in a variable
    # 3.25 make the output files for the readnorm (prepares bubble plot data) function
              # stores counts of Ts in fully edited
              # stores count of Ts in pre-edited

    # 3.26 this function will generate the data for bubble plots
         #BD 10/16: this may be old info: intermed2 contains all sequences that passed primer binning and T stripping filters
          # 3.26a this scaling will be used later for the sizes of the bubbles.  BD question:  is the data set used for this next part already normalized to RPMR or %?
                                     # seqcount is the sum of all the counts in the condensed identical reads file. The normalized counts are written out to the new file as NewCount by
                                     # taking the product of the non-normalized count and the scaling
        #3.26b This next step is using the intermed8 file to make a new dataset that associates the T count at each position with a type of editing event (insertion, deletion, incomplete insertion etc) 
                    #3.26c This next part is classifying the types of editing events for each 'level'.  The different levels classify the editing events in more detailed or less detailed ways to allow us to make different versions of the data presentation later
                    ## it is probably worth discussing if we care to keep all the different levels. If it makes the outputs more confusing we should just delete
    # 3.27 define variables for the cmp condense functions.
            # must match the annot values in the bubble plots output
    #3.28 code for the "CMP condense" 
    #I (BD) think this step is making a dictionary that is assigning counts to orders and nucleotids to prepare the bar graph or bubbleplot data
    # readnorm finalizes the output for the bubble plots, CMPcondense finalizes the output for bargraphs. Basically, it looks at the bubble plot data output from readnorm and counts the instances
    # of each type of editing event (which is different depending on the "level") at every non-u site. This uses only the rpmr values so across each level, the counts should sum to 1000000.
    # which so far as i can tell in my test runs, they do (barring the occasional +/- 2 from rounding)
                            # 3.28aUpdate the counts for the matching order
                                     # create a counter for each editing event
                         # 3.28b Create a new entry for the order
        #3.29 this turns the condensed_rows into a list
        # 3.30 This is making the bargraph output data file
            # 3.30a Write each condensed row to the output CSV
        #### write out alignments for just the cell line currently being investigated ###
        # store data in a list of tuples so it is easy to loop through and extract the top n designated in the input file
###### start of single cell line alignments ######
    ### this has been added to the 10/14 version of the script but is using the same functionality as the all cell lines alignments with some simplifications (ie no need for difference ordering)
    # determine scaling value to normalize read counts to reads per million reads
    # generate sequences with Ts between non-Ts
                    # Append the count before appending the character
                    # Convert count to string
            # Append the count before ending the loop
             # Convert count to string
    # create new sequence containing locations of ACG nucleotides and asterisk counts
    # t strip the sequence
    # find locations of ACG nucleotides and asterisk counts in non-T sequence
    # select out positions just 5' of Ts and counts of Ts at each site
    # concatenate the T locations and counts for each position
    # identify all T positions across all sequences
    # order the all_positions_list sequentially
    # create a new list where the T counts for every position (even if 0) are noted. All sublists are now the same length
    # chop out the t counts for each position into a new list
            # Initialize as None
             # add 1 to get characters after the letter
    # find maximum number of Ts for each position
        # consolidate the t counts for each position into a separate sublist
        # convert counts from strings to integers
        # find maximum number Ts for each position
    # store the difference in number of Ts at each site editing site
    # write out the aligned sequence
    # these indices keep track of what locations are getting Ts and - added
    # adds Ts or - to each T location identifier
                # Move to the next index for t_count
                # Move to the next index for dash_count
        # reset the indices if they exceed the length of t_count and dash_count. makes sure the t_counts and t_differences are applying to the correct location identifier
    # remove the location numbers while keeping letters and -
            # remove location numbers and change letters to uppercase
    # concatenate everything together for a final sequence with dashes and Ts
    # replaces Ts with Us
    # trim primers off sequences (except for a couple bases in the case of U editing in primer space)
    # acquire length of each sequence
    # create unique variable for pe and fe sequences with dashes and Ts. Necessary to reference for next loop
    # concatenate everything together for a final sequence with -, us, Us, and *
    # move deletions to the 3' end of the sequence
        # Use a regular expression to find all segments with the pattern * followed by one or more Us
    ##### end of second run #####
    ##### start of the alignment for the sequences subset by editing site depth #####
    # loop through all positions where there are Us are extract the indexes of those where there are differences (ie editing sites)
            # checks the t counts at each site to see which positions have differences ie editing sites
             # take final index representing final editing site
             # find the index represented the editing depth of interest
             # extract the non-T value that correlates to your editing depth of interest
          # extract number position of editing base depth
         # extract base for editing depth
    # find the index in each unprocessed sequence where the true editing depth is
    # find the index in the processed sequences where the true editing depth is
    # extract the portion of unprocessed sequence that is designated by the editing depth
    # extract the portion of sequence of interest from the list of processed sequences
    # select out only those portions of unprocessed sequences that are unique
    # select out only those portions of processed sequences that are unique
    # search for the portion in each sequence in each mutant
    # condense the information from unique portions list long
    # pair the unique portions and their respective percentages for each mutant
    # reformat list so it is sorted by sequence and not mutant
    # make a list of the portioned sequences in their original order
    # add normalized counts to list
    ## processing data for portioned sequence editing depth ##
    # extract T counts for each position based on our portion
    # another transformation...
    # update result3 to only contain U site counts for those positions in the portion of interest
    # create a list containing the last edited site by comparing T counts to PE reference. If no editing occurs, len of amplicon is used
    # create an arbitrary ranking system
    # zip the lists together
    # sort based on the first list (priority_list) in descending order, in case of ties, use amount of Us at that site
    # Extract the sorted items
            # make a list with just the sequences
    ## processing data for portioned sequence difference ##
    # extract the frequencies of the non-controls
    # sort based on the first list (priority_list) in descending order, in case of ties, use amount of Us at that site
    # Extract the sorted items
    # uport_depth_order_list_sorted = []
    # uport_dif_order_list_sorted = []
    # uport_dif_per_mut_sorted = []
        # uport_depth_order_list_sorted.append([item[3] for item in dep])
        # uport_dif_order_list_sorted.append([item[4] for item in dep])
        # uport_dif_per_mut_sorted.append([item[5] for item in dep])
    ##### end of the alignment for the sequences subset by editing site depth #####
    # take out sequences for final pe and fe alignments
    # create a list containing unique sequences, excluding pe and fe
    # add the final aligned seqs to each adapted_list sublist
    # take the processed sequences in their original order
    # add normalized read counts to each mutant sublist
    # take out the t counts in the pe sequence for use as a reference
    # create a list containing the last edited site by comparing T counts to PE reference. If no editing occurs, len of amplicon is used
    # create an arbitrary ranking system
    # zip the lists together
    # sort based on the first list (priority_list) in descending order, in case of ties, use amount of Us at that site
    # extract the sorted items
    # find the final order of the sequences when sorted by depth while maintaining input sequence order
    # extract the frequencies of the non-controls
    # extract the sorted items
    # dif_order_list_sorted = [item[4] for item in list_by_seq_sorted_pre]
    # dif_per_mut_sorted = [item[5] for item in list_by_seq_sorted_pre]
    # determine the lengths of each sequence
    # process the sequences so non-U stretches are pasted in as periods
    # extract the total percentages of sequences pulled for each cell line
    # repeat the above for absolute number of reads
    # repeat the above for reads per million reads
    ## repeat the above functions for the portioned sequences
    # repeat the above for absolute number of reads
    # repeat the above for reads per million reads
    # lists for sorting headers and data
    # create excel workbook
         # intialize list for the portioned tabs
         # store the sheet calls for the portions 
    # define some formats
           # Default color (black)
    # prepare some columnn headers for each tab containing sequences
    # write out whole sequence data to the first sheet in the workbook
        # Prepare the rich string with formatting
    # write out the portioned data
            # Prepare the rich string with formatting
                # write out the sequences with color coding to the appropriate cell
                # Write the rich string to the cell
    # write out period sequence data to the third sheet in the workbook
        # Prepare the rich string with formatting
            # write out the sequences with color coding to the appropriate cell
            # Write the rich string to the cell
    # fill out the QC tab
    # save the workbook
###### end of single cell line alignments ######
    # 3.31 run a series of functions in required order.  I (BD) am confused about what exactly this step is doing
        # you can think of a function in python like a command in excel (eg SUM() or STD()). It has predefined steps and can be applied to any variable that will operate within the rules of the
            # function. The functions you see above are just definitions of what each function does, they are not actually executing anything. The list below calls each function then feeds it
            # the input that goes through the pre-defined rules that are specified above. Sean can definitely explain this more thoroughly if you need
        # max wrote all of these as functions because originally everything was a small separate script before being integrated into the larger one (hence "operation all scripts")
        # because many of these functions rely on others (eg you need the bubble plot data before you can generate the bar graph data), they are organized as follows
        # i probably wouldn't have written out all the functions this way since they are only used once, but this organization works so i havent messed with it
    # info: 'seqcount' is a variable
    # info: the following are functions.# Tcountseq describes locations of Ts relative to A/C/Gs using *
    # info: 'Tcountan' seems to me (BD) similar to Tcountseq but it replaces the non-numeric characters with numbers
    # info: 'csvpatternconv' seems to be reorganizing using a dictionary based on 'nucleotide', 'position' and 'order'
    # info: 'readnorm' is involved in scaling data and uses seqcount.  I don't think I did a good job of explaining this one.
    # info: 'CMPconsense' is another dictionary-using spreadsheet organizing function
    # 3.32 create a dictionary to store the QC information
    # 3.33 writing summary document
    # 3.34 format table headers
    # 3.35 format the data
    # info: more Sankey data. Define the filename for the text file
    # Open the file in write mode
        # Iterate over the data
            # Write each item in the specified format to the file
    # 3.36 generate folder to store bar graph figures later
    # 3.37 generate folder to store bar graph figures later
#### end of per cell line loop (called part 3 here by BD) ####
#### Part 4: beginning processing data for all cell lines ####
# 4.1 create a folder for inter-cell line analyses and notify user if the folder already exists
# 4.2 create a folder within the previous folder for the difference plots and notify user
# 4.3 this function is preparing to make the R plots (R is run via the command line)
# this is just to make sure the lists input from python to R behave because they handle them in different formats
# these are all the lists initialized at the beginning of the big loop (pt 3)
# 4.4 run the R script to generate bar plots
# 4.5 run the R script to generate bubble plots
# 4.6 run the R script to generate difference plots
# 4.7 run the R script to generate editing extent plots
# 4.8 run the R script to generate editing event plots This should be the last step that is done using R
# 4.9 correct for case sensitivity and extra spaces in sort choice
# 4.10 store data in a list of tuples so it is easy to loop through and extract the top n designated in the input file
# 4.11 remove the top row containing columns labels
# 4.12 find the total number of sequence reads for each cell line
# 4.13 determine scaling value to normalize read counts to reads per million reads
# 4.14 extract top n sequences and related counts from the contextual aligned reads csv file
# 4.15 take the top n elements defined by the variable depth
# 4.16 make a list of the unique top sequences
# 4.17 create a list where the top sequences across all mutants are matched to each individual mutant sublist
# 4.18 create a list describing the number of occurrences for each of the top sequences we are looking at. Used for fasta output
# 4.19 combine all sequences (including pe and fe) into one list
# 4.20 generate sequences with numbers (number of Ts) between non-Ts.  I think this will be used for making alignments later (yes it will help us make sure everything is the same length)
                # info: Append the count before appending the character
        # info: Append the count before ending the loop
# 4.22 create new sequence containing locations of ACG nucleotides and asterisk counts
# 4.23 find locations of ACG nucleotides and asterisk counts in non-T sequence
# 4.24 select out positions just 5' of Ts and counts of Ts at each site
            # Check if there's an item before pos
# 4.25 concatenate the T locations and counts for each position
# 4.26 identify all T positions across all sequences
# 4.27  order the all_positions_list sequentially
# 4.28 create a new list where the T counts for every position (even if 0) are noted. All sublists are now the same length
# 4.29 chop out the t counts for each position into a new list
        # Initialize as None
        # add 1 to get characters after the letter
# 4.30 find maximum number of Ts for each position
     # consolidate the t counts for each position into a separate sublist
    # convert counts from strings to integers
     # find maximum number Ts for each position
# 4.31 store the difference in number of Ts at each site editing site
# 4.32 write out the aligned sequence
     # info: these indices keep track of what locations are getting Ts and - added
# 4.33 adds Ts or - to each T location identifier
            # info: Move to the next index for t_count
            # info: Move to the next index for dash_count
    # info: reset the indices if they exceed the length of t_count and dash_count. makes sure the t_counts and t_differences are applying to the correct location identifier
# 4.34 remove the location numbers while keeping letters and -
        # remove location numbers and change letters to uppercase
# 4.35 concatenate everything together for a final sequence with dashes and Ts
# 4.36 replaces Ts with Us
# 4.37 trim primers off sequences (except for a couple bases in the case of U editing in primer space)
# 4.38 acquire length of each sequence
# 4.39 create unique variable for pe and fe sequences with dashes and Ts. Necessary to reference for next loop
# this just changes some of the characters so we can differentiate deletions vs missing Us, insertions vs Us that were already there, etc
# and we determine what is happening at these sites by referencing back to what is in the processed pe and fe sequences
#info: this step seems to transform the alignment data into a couple of new datasets
# 4.40 for making the alignments, concatenate everything together for a final sequence with -, us, Us, and *
# 4.41 move deletions to the 3' end of the sequence
    # info: Use a regular expression to find all segments with the pattern * followed by one or more Us
### 4.42 use map to move the asterisks (?) in all sequences in the list ###
# this is not actually used for anything, I was just playing around with map commands since it is faster in R and I wanted to see if it had a similar functionality in python, we can delete
##### end of second run #####
#####4.43 this section does something similar as previous to make a different alignment. Start of the alignment for the sequences subset by editing site depth #####
# 4.43a : loop through all positions where there are Us are extract the indexes of those where there are differences (ie editing sites)
     # info: checks the t counts at each site to see which positions have differences ie editing sites
     # info: take final index representing final editing site
      # info: find the index represented the editing depth of interest
      # info: extract the non-T value that correlates to your editing depth of interest
      # 4.43b: extract number position of editing base depth
      # info: extract base for editing depth
# 4.43c: find the index in each unprocessed sequence where the true editing depth is
# 4.43d: find the index in the processed sequences where the true editing depth is
# 4.43e: extract the portion of unprocessed sequence that is designated by the editing depth
# 4.43f: extract the portion of sequence of interest from the list of processed sequences
# 4.43g: select out only those portions of unprocessed sequences that are unique
# 4.43h: select out only those portions of processed sequences that are unique
# 4.43i search for the portion in each sequence in each mutant
# 4.43j condense the information from unique portions list long
# 4.43k pair the unique portions and their respective percentages for each mutant
# 4.43l reformat list so it is sorted by sequence and not mutant
# 4.43m make a list of the portioned sequences in their original order
# 4.43n add normalized counts to list ###### left off here ######
## 4.44 processing data for portioned sequence editing depth.  This goes with the sequences in 4.43 ##
# 4.44a extract T counts for each position based on our portion
# 4.44b another transformation...
# 4.44c update result3 to only contain U site counts for those positions in the portion of interest
# 4.44d create a list containing the last edited site by comparing T counts to PE reference. If no editing occurs, len of amplicon is used
# 4.44e create an arbitrary ranking system
# 4.44f zip the lists together
# 4.44g sort based on the first list (priority_list) in descending order, in case of ties, use amount of Us at that site
# 4.44h Extract the sorted items
# 4.44i determine the order of sequences in the case that you sort by depth
## 4.45 processing data for portioned sequence difference ##
# 4.45a extract the normalized frequencies of each sequence for your control sample
# 4.45b extract the frequencies of the non-controls
# 4.45c find the absolute difference between the number of reads in the control sample and the rest. Also determine total difference across mutants
#info: there seem to be a lot of data transformations for 4.45c
# 4.45d determine the order of sequences in the case that you sort by difference #####Left off here######
# 4.45e this looks like another transformation of the data in the last step
# 4.45f sorting the sequences by 'depth' if that is what the user selected
    # info: sort based on the first list (priority_list) in descending order, in case of ties, use amount of Us at that site
    # info: Extract the sorted items
# 4.45g sorting the sequences by 'difference' if that is what the user chose instead of 'depth'
##### end of the alignment for the sequences subset by editing site depth #####
####### 4.46 BD: what is this next alignment?######
## this isnt aligning anything new, it is just sorting the whole length sequences instead of just the portions. The code dealing with the portioned sequences is in the middle of everything
## because I wrote the portion alignments, then we decided to order stuff, so then the whole sequence ordering just got dumped onto the end here
# 4.46a take out sequences for final pe and fe alignments
# 4.46b create a list containing unique sequences, excluding pe and fe
# 4.46c add the final aligned seqs to each adapted_list sublist
# 4.46d take the processed sequences in their original order
# 4.46e add normalized read counts to each mutant sublist
# 4.46f take out the t counts in the pe sequence for use as a reference
# 4.46g create a list containing the last edited site by comparing T counts to PE reference. If no editing occurs, len of amplicon is used
# 4.46h create an arbitrary ranking system
# 4.46i zip the lists together
# 4.46j sort based on the first list (priority_list) in descending order, in case of ties, use amount of Us at that site
# 4.46k Extract the sorted items
# 4.46l find the final order of the sequences when sorted by depth while maintaining input sequence order
# 4.46m extract the normalized frequencies of each sequence for your control sample
# 4.46n extract the frequencies of the non-controls
# 4.46o find the absolute difference between the number of reads in the control sample and the rest. Also determine total difference across mutants
# 4.46p determine the order of sequences in the case that you sort by difference
#####  4.47 below is more sorting by depth or difference. We already went over this so I'm going to be brief.
# info: start of sorting by editing depth #####
    # info: zip the lists together and sort everything by depth
    # info: sort based on the first list (priority_list) in descending order, in case of ties, use amount of Us at that site
    # info: Extract the sorted items
##### 4.47b end of sorting by editing depth, start of sorting by difference between mutant and control #####
    # info: zip the lists together and sort everything by difference
    # info: sort the list based on total difference between sequences
##### end of sorting by difference between mutant and control #####
# 4.48 determine the lengths of each sequence
# 4.49 process the sequences so non-U stretches are pasted in as periods
# 4.50 extract the total percentages of sequences pulled for each cell line
# 4.51repeat the above for absolute number of reads
# 4.52 repeat the above for reads per million reads
## 4.53 repeat the above functions for the portioned sequences
# 4.54repeat the above for absolute number of reads
# 4.55 repeat the above for reads per million reads
# 4.56 lists for sorting headers and data
# 4.57 create excel workbook
   # intialize list for the portioned tabs
   # store the sheet calls for the portions
# 4.58 define some formats
# 4.59 prepare some columnn headers for each tab containing sequences
# 4.60 write out whole sequence data to the first sheet in the workbook
    # 4.60a Prepare the rich string with formatting
        # 4.60b write out the sequences with color coding to the appropriate cell
        # 4.60c Write the rich string to the cell
# 4.61 write out the portioned data
        # Prepare the rich string with formatting
            # write out the sequences with color coding to the appropriate cell
            # Write the rich string to the cell
# 4.62 write out period sequence data to the third sheet in the workbook
    # 4.62a Prepare the rich string with formatting
        # 4.62bwrite out the sequences with color coding to the appropriate cell
        # 4.62cWrite the rich string to the cell
# 4.63 fill out the QC tab
# 4.64 add the difference matrices to a new sheet
# 4.65 save the workbook
# 4.66 generate sequence diversity data
# 4.67 create sequence diversity excel book
# 4.68 write the headers out to the file
# 4.69 write the data out to the file
##END####
